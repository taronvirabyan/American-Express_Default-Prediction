{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":35332,"databundleVersionId":3723648,"sourceType":"competition"},{"sourceId":3727003,"sourceType":"datasetVersion","datasetId":2213609}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd \nimport optuna\nimport lightgbm as lgb\nimport gc\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier, early_stopping, log_evaluation\nfrom xgboost import XGBClassifier# data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-22T14:19:29.868408Z","iopub.execute_input":"2024-05-22T14:19:29.868827Z","iopub.status.idle":"2024-05-22T14:19:30.329778Z","shell.execute_reply.started":"2024-05-22T14:19:29.868796Z","shell.execute_reply":"2024-05-22T14:19:30.328655Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/amex-default-prediction/sample_submission.csv\n/kaggle/input/amex-default-prediction/train_data.csv\n/kaggle/input/amex-default-prediction/test_data.csv\n/kaggle/input/amex-default-prediction/train_labels.csv\n/kaggle/input/amexfeather/test_data_f32.ftr\n/kaggle/input/amexfeather/train_data.ftr\n/kaggle/input/amexfeather/train_data_f32.ftr\n/kaggle/input/amexfeather/test_data.ftr\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset_ = pd.read_feather('../input/amexfeather/train_data.ftr')","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:19:39.839537Z","iopub.execute_input":"2024-05-22T14:19:39.840121Z","iopub.status.idle":"2024-05-22T14:20:04.309278Z","shell.execute_reply.started":"2024-05-22T14:19:39.840088Z","shell.execute_reply":"2024-05-22T14:20:04.308025Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset_.groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T16:53:14.234985Z","iopub.execute_input":"2024-05-22T16:53:14.235423Z","iopub.status.idle":"2024-05-22T16:53:19.085328Z","shell.execute_reply.started":"2024-05-22T16:53:14.235388Z","shell.execute_reply":"2024-05-22T16:53:19.083987Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T16:52:57.259268Z","iopub.execute_input":"2024-05-22T16:52:57.259694Z","iopub.status.idle":"2024-05-22T16:52:58.395127Z","shell.execute_reply.started":"2024-05-22T16:52:57.259664Z","shell.execute_reply":"2024-05-22T16:52:58.393613Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"3033"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T16:53:24.267878Z","iopub.execute_input":"2024-05-22T16:53:24.268376Z","iopub.status.idle":"2024-05-22T16:53:24.301422Z","shell.execute_reply.started":"2024-05-22T16:53:24.268338Z","shell.execute_reply":"2024-05-22T16:53:24.299679Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"                                                          S_2       P_2  \\\ncustomer_ID                                                               \n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb... 2018-03-13  0.934570   \n00000fd6641609c6ece5454664794f0340ad84dddce9a26... 2018-03-25  0.880371   \n00001b22f846c82c51f6e3958ccd81970162bae8b007e80... 2018-03-12  0.880859   \n000041bdba6ecadd89a52d11886e8eaaec9325906c97233... 2018-03-29  0.621582   \n00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad... 2018-03-30  0.872070   \n\n                                                        D_39       B_1  \\\ncustomer_ID                                                              \n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.009117  0.009384   \n00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.178101  0.034698   \n00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.009705  0.004284   \n000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.001082  0.012566   \n00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.005573  0.007679   \n\n                                                         B_2       R_1  \\\ncustomer_ID                                                              \n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  1.007812  0.006104   \n00000fd6641609c6ece5454664794f0340ad84dddce9a26...  1.003906  0.006912   \n00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.812500  0.006451   \n000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  1.005859  0.007828   \n00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.815918  0.001247   \n\n                                                         S_3      D_41  \\\ncustomer_ID                                                              \n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.135010  0.001604   \n00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.165527  0.005550   \n00001b22f846c82c51f6e3958ccd81970162bae8b007e80...       NaN  0.003796   \n000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.287842  0.004532   \n00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...       NaN  0.000231   \n\n                                                         B_3  D_42  ...  \\\ncustomer_ID                                                         ...   \n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.007175   NaN  ...   \n00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.005070   NaN  ...   \n00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.007195   NaN  ...   \n000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.009941   NaN  ...   \n00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.005527   NaN  ...   \n\n                                                    D_137  D_138     D_139  \\\ncustomer_ID                                                                  \n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...    NaN    NaN  0.007187   \n00000fd6641609c6ece5454664794f0340ad84dddce9a26...    NaN    NaN  0.002981   \n00001b22f846c82c51f6e3958ccd81970162bae8b007e80...    NaN    NaN  0.007381   \n000041bdba6ecadd89a52d11886e8eaaec9325906c97233...    NaN    NaN  0.002705   \n00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...    NaN    NaN  0.002974   \n\n                                                       D_140     D_141  D_142  \\\ncustomer_ID                                                                     \n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.004234  0.005085    NaN   \n00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.007481  0.007874    NaN   \n00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.006622  0.000965    NaN   \n000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.006184  0.001899    NaN   \n00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.004162  0.005764    NaN   \n\n                                                       D_143     D_144  \\\ncustomer_ID                                                              \n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.005810  0.002970   \n00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.003284  0.003170   \n00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.002201  0.000834   \n000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.008186  0.005558   \n00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.008156  0.006943   \n\n                                                       D_145  target  \ncustomer_ID                                                           \n0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.008530       0  \n00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.008514       0  \n00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.003445       0  \n000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.002983       0  \n00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.000905       0  \n\n[5 rows x 190 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>S_2</th>\n      <th>P_2</th>\n      <th>D_39</th>\n      <th>B_1</th>\n      <th>B_2</th>\n      <th>R_1</th>\n      <th>S_3</th>\n      <th>D_41</th>\n      <th>B_3</th>\n      <th>D_42</th>\n      <th>...</th>\n      <th>D_137</th>\n      <th>D_138</th>\n      <th>D_139</th>\n      <th>D_140</th>\n      <th>D_141</th>\n      <th>D_142</th>\n      <th>D_143</th>\n      <th>D_144</th>\n      <th>D_145</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>customer_ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a</th>\n      <td>2018-03-13</td>\n      <td>0.934570</td>\n      <td>0.009117</td>\n      <td>0.009384</td>\n      <td>1.007812</td>\n      <td>0.006104</td>\n      <td>0.135010</td>\n      <td>0.001604</td>\n      <td>0.007175</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.007187</td>\n      <td>0.004234</td>\n      <td>0.005085</td>\n      <td>NaN</td>\n      <td>0.005810</td>\n      <td>0.002970</td>\n      <td>0.008530</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5</th>\n      <td>2018-03-25</td>\n      <td>0.880371</td>\n      <td>0.178101</td>\n      <td>0.034698</td>\n      <td>1.003906</td>\n      <td>0.006912</td>\n      <td>0.165527</td>\n      <td>0.005550</td>\n      <td>0.005070</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002981</td>\n      <td>0.007481</td>\n      <td>0.007874</td>\n      <td>NaN</td>\n      <td>0.003284</td>\n      <td>0.003170</td>\n      <td>0.008514</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>00001b22f846c82c51f6e3958ccd81970162bae8b007e80662ef27519fcc18c1</th>\n      <td>2018-03-12</td>\n      <td>0.880859</td>\n      <td>0.009705</td>\n      <td>0.004284</td>\n      <td>0.812500</td>\n      <td>0.006451</td>\n      <td>NaN</td>\n      <td>0.003796</td>\n      <td>0.007195</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.007381</td>\n      <td>0.006622</td>\n      <td>0.000965</td>\n      <td>NaN</td>\n      <td>0.002201</td>\n      <td>0.000834</td>\n      <td>0.003445</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>000041bdba6ecadd89a52d11886e8eaaec9325906c9723355abb5ca523658edc</th>\n      <td>2018-03-29</td>\n      <td>0.621582</td>\n      <td>0.001082</td>\n      <td>0.012566</td>\n      <td>1.005859</td>\n      <td>0.007828</td>\n      <td>0.287842</td>\n      <td>0.004532</td>\n      <td>0.009941</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002705</td>\n      <td>0.006184</td>\n      <td>0.001899</td>\n      <td>NaN</td>\n      <td>0.008186</td>\n      <td>0.005558</td>\n      <td>0.002983</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad51ca8b8c4a24cefed</th>\n      <td>2018-03-30</td>\n      <td>0.872070</td>\n      <td>0.005573</td>\n      <td>0.007679</td>\n      <td>0.815918</td>\n      <td>0.001247</td>\n      <td>NaN</td>\n      <td>0.000231</td>\n      <td>0.005527</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002974</td>\n      <td>0.004162</td>\n      <td>0.005764</td>\n      <td>NaN</td>\n      <td>0.008156</td>\n      <td>0.006943</td>\n      <td>0.000905</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 190 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"categorical_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n\nnum_cols = [col for col in train_dataset.columns if col not in categorical_cols + [\"target\"]]\n\nprint(f'Total number of features: {1}')\nprint(f'Total number of categorical features: {len(categorical_cols)}')\nprint(f'Total number of continuos features: {len(num_cols)}')","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:22:07.538416Z","iopub.execute_input":"2024-05-22T14:22:07.538857Z","iopub.status.idle":"2024-05-22T14:22:07.547160Z","shell.execute_reply.started":"2024-05-22T14:22:07.538823Z","shell.execute_reply":"2024-05-22T14:22:07.546010Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Total number of features: 1\nTotal number of categorical features: 11\nTotal number of continuos features: 178\n","output_type":"stream"}]},{"cell_type":"code","source":"NaN_Val = np.array(train_dataset.isnull().sum())\nNaN_prec = np.array((train_dataset.isnull().sum() * 100 / len(train_dataset)).round(2))\nNaN_Col = pd.DataFrame([np.array(list(train_dataset.columns)).T,NaN_Val.T,NaN_prec.T,np.array(list(train_dataset.dtypes)).T], index=['Features','Num of Missing values','Percentage','DataType']\n).transpose()\npd.set_option('display.max_rows', None)\nNaN_Col","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:35:09.452213Z","iopub.execute_input":"2024-05-22T14:35:09.454682Z","iopub.status.idle":"2024-05-22T14:35:10.346610Z","shell.execute_reply.started":"2024-05-22T14:35:09.454621Z","shell.execute_reply":"2024-05-22T14:35:10.345003Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"    Features Num of Missing values Percentage        DataType\n0        S_2                     0        0.0  datetime64[ns]\n1        P_2                  2969       0.65         float16\n2       D_39                     0        0.0         float16\n3        B_1                     0        0.0         float16\n4        B_2                    31       0.01         float16\n5        R_1                     0        0.0         float16\n6        S_3                 84970      18.52         float16\n7       D_41                    31       0.01         float16\n8        B_3                    31       0.01         float16\n9       D_42                399003      86.95         float16\n10      D_43                134322      29.27         float16\n11      D_44                 22295       4.86         float16\n12       B_4                     0        0.0         float16\n13      D_45                    31       0.01         float16\n14       B_5                     0        0.0         float16\n15       R_2                     0        0.0         float16\n16      D_46                 95123      20.73         float16\n17      D_47                     0        0.0         float16\n18      D_48                 57992      12.64         float16\n19      D_49                407150      88.72         float16\n20       B_6                    40       0.01         float16\n21       B_7                     0        0.0         float16\n22       B_8                  4091       0.89         float16\n23      D_50                262235      57.14         float16\n24      D_51                     0        0.0         float16\n25       B_9                     0        0.0         float16\n26       R_3                     0        0.0         float16\n27      D_52                  1240       0.27         float16\n28       P_3                 22220       4.84         float16\n29      B_10                     0        0.0         float16\n30      D_53                325932      71.02         float16\n31       S_5                     0        0.0         float16\n32      B_11                     0        0.0         float16\n33       S_6                     0        0.0         float16\n34      D_54                    31       0.01         float16\n35       R_4                     0        0.0         float16\n36       S_7                 84970      18.52         float16\n37      B_12                     0        0.0         float16\n38       S_8                     0        0.0         float16\n39      D_55                 30377       6.62         float16\n40      D_56                244734      53.33         float16\n41      B_13                  1563       0.34         float16\n42       R_5                     0        0.0         float16\n43      D_58                     0        0.0         float16\n44       S_9                183858      40.06         float16\n45      B_14                     0        0.0         float16\n46      D_59                  4086       0.89         float16\n47      D_60                     0        0.0         float16\n48      D_61                 48348      10.54         float16\n49      B_15                   612       0.13         float16\n50      S_11                     0        0.0         float16\n51      D_62                 58953      12.85         float16\n52      D_63                     0        0.0        category\n53      D_64                     0        0.0        category\n54      D_65                     0        0.0         float16\n55      B_16                    31       0.01         float16\n56      B_17                244471      53.27         float16\n57      B_18                     0        0.0         float16\n58      B_19                    31       0.01         float16\n59      D_66                406331      88.54        category\n60      B_20                    31       0.01         float16\n61      D_68                  9012       1.96        category\n62      S_12                     0        0.0         float16\n63       R_6                     0        0.0         float16\n64      S_13                     0        0.0         float16\n65      B_21                     0        0.0         float16\n66      D_69                  6365       1.39         float16\n67      B_22                    31       0.01         float16\n68      D_70                  4196       0.91         float16\n69      D_71                     0        0.0         float16\n70      D_72                  1216       0.26         float16\n71      S_15                     0        0.0         float16\n72      B_23                     0        0.0         float16\n73      D_73                454674      99.08         float16\n74       P_4                     0        0.0         float16\n75      D_74                  1701       0.37         float16\n76      D_75                     0        0.0         float16\n77      D_76                409597      89.25         float16\n78      B_24                     0        0.0         float16\n79       R_7                     0        0.0         float16\n80      D_77                213837       46.6         float16\n81      B_25                   612       0.13         float16\n82      B_26                    31       0.01         float16\n83      D_78                 22295       4.86         float16\n84      D_79                  2795       0.61         float16\n85       R_8                     0        0.0         float16\n86       R_9                431960      94.13         float16\n87      S_16                     0        0.0         float16\n88      D_80                  1701       0.37         float16\n89      R_10                     0        0.0         float16\n90      R_11                     0        0.0         float16\n91      B_27                    31       0.01         float16\n92      D_81                  1190       0.26         float16\n93      D_82                343295      74.81         float16\n94      S_17                     0        0.0         float16\n95      R_12                     0        0.0         float16\n96      B_28                     0        0.0         float16\n97      R_13                     0        0.0         float16\n98      D_83                  6365       1.39         float16\n99      R_14                     0        0.0         float16\n100     R_15                     0        0.0         float16\n101     D_84                  1240       0.27         float16\n102     R_16                     0        0.0         float16\n103     B_29                431589      94.05         float16\n104     B_30                    31       0.01        category\n105     S_18                     0        0.0         float16\n106     D_86                     0        0.0         float16\n107     D_87                458268      99.86         float16\n108     R_17                     0        0.0         float16\n109     R_18                     0        0.0         float16\n110     D_88                458086      99.82         float16\n111     B_31                     0        0.0         float16\n112     S_19                     0        0.0         float16\n113     R_19                     0        0.0         float16\n114     B_32                     0        0.0         float16\n115     S_20                     0        0.0         float16\n116     R_20                     0        0.0         float16\n117     R_21                     0        0.0         float16\n118     B_33                    31       0.01         float16\n119     D_89                  1240       0.27         float16\n120     R_22                     0        0.0         float16\n121     R_23                     0        0.0         float16\n122     D_91                 12807       2.79         float16\n123     D_92                     0        0.0         float16\n124     D_93                     0        0.0         float16\n125     D_94                     0        0.0         float16\n126     R_24                     0        0.0         float16\n127     R_25                     0        0.0         float16\n128     D_96                     0        0.0         float16\n129     S_22                  1767       0.39         float16\n130     S_23                    44       0.01         float16\n131     S_24                  1734       0.38         float16\n132     S_25                  1421       0.31         float16\n133     S_26                     0        0.0         float16\n134    D_102                     0        0.0         float16\n135    D_103                  2830       0.62         float16\n136    D_104                  2830       0.62         float16\n137    D_105                245602      53.52         float16\n138    D_106                407265      88.75         float16\n139    D_107                  2830       0.62         float16\n140     B_36                     0        0.0         float16\n141     B_37                     0        0.0         float16\n142     R_26                407770      88.86         float16\n143     R_27                 28736       6.26         float16\n144     B_38                    31       0.01        category\n145    D_108                456286      99.43         float16\n146    D_109                    31       0.01         float16\n147    D_110                455235       99.2         float16\n148    D_111                455235       99.2         float16\n149     B_39                454808      99.11         float16\n150    D_112                    31       0.01         float16\n151     B_40                     0        0.0         float16\n152     S_27                117166      25.53         float16\n153    D_113                  6017       1.31         float16\n154    D_114                  6017       1.31        category\n155    D_115                  6017       1.31         float16\n156    D_116                  6017       1.31        category\n157    D_117                  6017       1.31        category\n158    D_118                  6017       1.31         float16\n159    D_119                  6017       1.31         float16\n160    D_120                  6017       1.31        category\n161    D_121                  6017       1.31         float16\n162    D_122                  6017       1.31         float16\n163    D_123                  6017       1.31         float16\n164    D_124                  6017       1.31         float16\n165    D_125                  6017       1.31         float16\n166    D_126                     0        0.0        category\n167    D_127                     0        0.0         float16\n168    D_128                  2830       0.62         float16\n169    D_129                  2830       0.62         float16\n170     B_41                     0        0.0         float16\n171     B_42                452771      98.66         float16\n172    D_130                  2830       0.62         float16\n173    D_131                  2830       0.62         float16\n174    D_132                407153      88.72         float16\n175    D_133                     0        0.0         float16\n176     R_28                     0        0.0         float16\n177    D_134                442518      96.43         float16\n178    D_135                442518      96.43         float16\n179    D_136                442518      96.43         float16\n180    D_137                442518      96.43         float16\n181    D_138                442518      96.43         float16\n182    D_139                  2830       0.62         float16\n183    D_140                     0        0.0         float16\n184    D_141                  2830       0.62         float16\n185    D_142                378598       82.5         float16\n186    D_143                  2830       0.62         float16\n187    D_144                     0        0.0         float16\n188    D_145                  2830       0.62         float16\n189   target                     0        0.0           int64","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Features</th>\n      <th>Num of Missing values</th>\n      <th>Percentage</th>\n      <th>DataType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>S_2</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>datetime64[ns]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P_2</td>\n      <td>2969</td>\n      <td>0.65</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>D_39</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B_1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B_2</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>R_1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>S_3</td>\n      <td>84970</td>\n      <td>18.52</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>D_41</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>B_3</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>D_42</td>\n      <td>399003</td>\n      <td>86.95</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>D_43</td>\n      <td>134322</td>\n      <td>29.27</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>D_44</td>\n      <td>22295</td>\n      <td>4.86</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>B_4</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>D_45</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>B_5</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>R_2</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>D_46</td>\n      <td>95123</td>\n      <td>20.73</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>D_47</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>D_48</td>\n      <td>57992</td>\n      <td>12.64</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>D_49</td>\n      <td>407150</td>\n      <td>88.72</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>B_6</td>\n      <td>40</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>B_7</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>B_8</td>\n      <td>4091</td>\n      <td>0.89</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>D_50</td>\n      <td>262235</td>\n      <td>57.14</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>D_51</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>B_9</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>R_3</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>D_52</td>\n      <td>1240</td>\n      <td>0.27</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>P_3</td>\n      <td>22220</td>\n      <td>4.84</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>B_10</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>D_53</td>\n      <td>325932</td>\n      <td>71.02</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>S_5</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>B_11</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>S_6</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>D_54</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>R_4</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>S_7</td>\n      <td>84970</td>\n      <td>18.52</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>B_12</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>S_8</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>D_55</td>\n      <td>30377</td>\n      <td>6.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>D_56</td>\n      <td>244734</td>\n      <td>53.33</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>B_13</td>\n      <td>1563</td>\n      <td>0.34</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>R_5</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>D_58</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>S_9</td>\n      <td>183858</td>\n      <td>40.06</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>B_14</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>D_59</td>\n      <td>4086</td>\n      <td>0.89</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>D_60</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>D_61</td>\n      <td>48348</td>\n      <td>10.54</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>B_15</td>\n      <td>612</td>\n      <td>0.13</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>S_11</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>D_62</td>\n      <td>58953</td>\n      <td>12.85</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>D_63</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>D_64</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>D_65</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>B_16</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>B_17</td>\n      <td>244471</td>\n      <td>53.27</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>B_18</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>B_19</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>D_66</td>\n      <td>406331</td>\n      <td>88.54</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>B_20</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>D_68</td>\n      <td>9012</td>\n      <td>1.96</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>S_12</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>R_6</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>S_13</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>B_21</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>D_69</td>\n      <td>6365</td>\n      <td>1.39</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>B_22</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>D_70</td>\n      <td>4196</td>\n      <td>0.91</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>D_71</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>D_72</td>\n      <td>1216</td>\n      <td>0.26</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>S_15</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>B_23</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>D_73</td>\n      <td>454674</td>\n      <td>99.08</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>P_4</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>D_74</td>\n      <td>1701</td>\n      <td>0.37</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>D_75</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>D_76</td>\n      <td>409597</td>\n      <td>89.25</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>B_24</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>R_7</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>D_77</td>\n      <td>213837</td>\n      <td>46.6</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>B_25</td>\n      <td>612</td>\n      <td>0.13</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>B_26</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>D_78</td>\n      <td>22295</td>\n      <td>4.86</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>D_79</td>\n      <td>2795</td>\n      <td>0.61</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>R_8</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>R_9</td>\n      <td>431960</td>\n      <td>94.13</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>S_16</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>D_80</td>\n      <td>1701</td>\n      <td>0.37</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>R_10</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>R_11</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>B_27</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>D_81</td>\n      <td>1190</td>\n      <td>0.26</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>D_82</td>\n      <td>343295</td>\n      <td>74.81</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>S_17</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>R_12</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>B_28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>R_13</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>D_83</td>\n      <td>6365</td>\n      <td>1.39</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>R_14</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>R_15</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>D_84</td>\n      <td>1240</td>\n      <td>0.27</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>R_16</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>B_29</td>\n      <td>431589</td>\n      <td>94.05</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>B_30</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>S_18</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>D_86</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>D_87</td>\n      <td>458268</td>\n      <td>99.86</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>R_17</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>R_18</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>D_88</td>\n      <td>458086</td>\n      <td>99.82</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>B_31</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>S_19</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>R_19</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>B_32</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>S_20</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>R_20</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>R_21</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>B_33</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>D_89</td>\n      <td>1240</td>\n      <td>0.27</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>R_22</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>R_23</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>D_91</td>\n      <td>12807</td>\n      <td>2.79</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>D_92</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>D_93</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>D_94</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>R_24</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>R_25</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>D_96</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>S_22</td>\n      <td>1767</td>\n      <td>0.39</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>S_23</td>\n      <td>44</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>S_24</td>\n      <td>1734</td>\n      <td>0.38</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>S_25</td>\n      <td>1421</td>\n      <td>0.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>S_26</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>D_102</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>D_103</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>D_104</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>D_105</td>\n      <td>245602</td>\n      <td>53.52</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>D_106</td>\n      <td>407265</td>\n      <td>88.75</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>D_107</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>B_36</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>B_37</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>R_26</td>\n      <td>407770</td>\n      <td>88.86</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>R_27</td>\n      <td>28736</td>\n      <td>6.26</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>B_38</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>D_108</td>\n      <td>456286</td>\n      <td>99.43</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>D_109</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>D_110</td>\n      <td>455235</td>\n      <td>99.2</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>D_111</td>\n      <td>455235</td>\n      <td>99.2</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>B_39</td>\n      <td>454808</td>\n      <td>99.11</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>D_112</td>\n      <td>31</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>B_40</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>152</th>\n      <td>S_27</td>\n      <td>117166</td>\n      <td>25.53</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>D_113</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>D_114</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>D_115</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>D_116</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>157</th>\n      <td>D_117</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>D_118</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>159</th>\n      <td>D_119</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>160</th>\n      <td>D_120</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>D_121</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>162</th>\n      <td>D_122</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>163</th>\n      <td>D_123</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>D_124</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>D_125</td>\n      <td>6017</td>\n      <td>1.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>166</th>\n      <td>D_126</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>167</th>\n      <td>D_127</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>168</th>\n      <td>D_128</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>169</th>\n      <td>D_129</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>B_41</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>B_42</td>\n      <td>452771</td>\n      <td>98.66</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>D_130</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>D_131</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>D_132</td>\n      <td>407153</td>\n      <td>88.72</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>D_133</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>R_28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>D_134</td>\n      <td>442518</td>\n      <td>96.43</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>D_135</td>\n      <td>442518</td>\n      <td>96.43</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>D_136</td>\n      <td>442518</td>\n      <td>96.43</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>D_137</td>\n      <td>442518</td>\n      <td>96.43</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>D_138</td>\n      <td>442518</td>\n      <td>96.43</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>182</th>\n      <td>D_139</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>D_140</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>D_141</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>D_142</td>\n      <td>378598</td>\n      <td>82.5</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>D_143</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>D_144</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>D_145</td>\n      <td>2830</td>\n      <td>0.62</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>189</th>\n      <td>target</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>int64</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = train_dataset.drop(['S_2','D_66','D_42','D_49','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:35:30.443004Z","iopub.execute_input":"2024-05-22T14:35:30.443465Z","iopub.status.idle":"2024-05-22T14:35:30.876817Z","shell.execute_reply.started":"2024-05-22T14:35:30.443430Z","shell.execute_reply":"2024-05-22T14:35:30.875780Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"selected_col = np.array(['P_2','S_3','B_2','D_41','D_43','B_3','D_44','D_45','D_46','D_48','D_50','D_53','S_7','D_56','S_9','B_6','B_8','D_52','P_3','D_54','D_55','B_13','D_59','D_61','B_15','D_62','B_16','B_17','D_77','B_19','B_20','D_69','B_22','D_70','D_72','D_74','R_7','B_25','B_26','D_78','D_79','D_80','B_27','D_81','R_12','D_82','D_105','S_27','D_83','R_14','D_84','D_86','R_20','B_33','D_89','D_91','S_22','S_23','S_24','S_25','S_26','D_102','D_103','D_104','D_107','B_37','R_27','D_109','D_112','B_40','D_113','D_115','D_118','D_119','D_121','D_122','D_123','D_124','D_125','D_128','D_129','B_41','D_130','D_131','D_133','D_139','D_140','D_141','D_143','D_144','D_145'])\n\nfor col in selected_col:\n    train_dataset[col] = train_dataset[col].fillna(train_dataset[col].median())","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:36:34.894738Z","iopub.execute_input":"2024-05-22T14:36:34.895152Z","iopub.status.idle":"2024-05-22T14:36:37.000667Z","shell.execute_reply.started":"2024-05-22T14:36:34.895120Z","shell.execute_reply":"2024-05-22T14:36:36.999577Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"selcted_col2 = np.array(['D_68','B_30','B_38','D_64','D_114','D_116','D_117','D_120','D_126'])\n\nfor col2 in selcted_col2:\n    train_dataset[col2] =  train_dataset[col2].fillna(train_dataset[col2].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:36:45.756630Z","iopub.execute_input":"2024-05-22T14:36:45.757060Z","iopub.status.idle":"2024-05-22T14:36:45.806085Z","shell.execute_reply.started":"2024-05-22T14:36:45.757026Z","shell.execute_reply":"2024-05-22T14:36:45.804847Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"selcted_col2","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:37:07.599131Z","iopub.execute_input":"2024-05-22T14:37:07.599541Z","iopub.status.idle":"2024-05-22T14:37:07.608023Z","shell.execute_reply.started":"2024-05-22T14:37:07.599510Z","shell.execute_reply":"2024-05-22T14:37:07.606669Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array(['D_68', 'B_30', 'B_38', 'D_64', 'D_114', 'D_116', 'D_117', 'D_120',\n       'D_126'], dtype='<U5')"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset_ = pd.read_feather('../input/amexfeather/test_data.ftr')","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:37:28.601432Z","iopub.execute_input":"2024-05-22T14:37:28.601918Z","iopub.status.idle":"2024-05-22T14:38:19.321521Z","shell.execute_reply.started":"2024-05-22T14:37:28.601886Z","shell.execute_reply":"2024-05-22T14:38:19.320209Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test_dataset = test_dataset_.groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:39:50.524675Z","iopub.execute_input":"2024-05-22T14:39:50.525125Z","iopub.status.idle":"2024-05-22T14:39:58.013124Z","shell.execute_reply.started":"2024-05-22T14:39:50.525093Z","shell.execute_reply":"2024-05-22T14:39:58.012082Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-02-14T22:47:24.040575Z","iopub.execute_input":"2024-02-14T22:47:24.041910Z","iopub.status.idle":"2024-02-14T22:47:24.117907Z","shell.execute_reply.started":"2024-02-14T22:47:24.041854Z","shell.execute_reply":"2024-02-14T22:47:24.116611Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"485"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:40:04.930494Z","iopub.execute_input":"2024-05-22T14:40:04.930929Z","iopub.status.idle":"2024-05-22T14:40:04.963594Z","shell.execute_reply.started":"2024-05-22T14:40:04.930895Z","shell.execute_reply":"2024-05-22T14:40:04.962278Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                                          S_2       P_2  \\\ncustomer_ID                                                               \n00000469ba478561f23a92a868bd366de6f6527a684c9a2... 2019-10-12  0.568848   \n00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397... 2019-04-15  0.841309   \n0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5... 2019-10-16  0.697754   \n00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf... 2019-04-22  0.513184   \n00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a... 2019-10-22  0.254395   \n\n                                                        D_39       B_1  \\\ncustomer_ID                                                              \n00000469ba478561f23a92a868bd366de6f6527a684c9a2...  0.121399  0.010780   \n00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397...  0.126465  0.016556   \n0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5...  0.002724  0.001485   \n00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf...  0.324707  0.149536   \n00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a...  0.768066  0.563477   \n\n                                                         B_2       R_1  \\\ncustomer_ID                                                              \n00000469ba478561f23a92a868bd366de6f6527a684c9a2...  1.009766  0.006924   \n00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397...  1.008789  0.009712   \n0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5...  0.810059  0.002621   \n00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf...  0.205688  0.002277   \n00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a...  0.038025  0.502930   \n\n                                                         S_3      D_41  \\\ncustomer_ID                                                              \n00000469ba478561f23a92a868bd366de6f6527a684c9a2...  0.149414  0.000396   \n00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397...  0.112183  0.006191   \n0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5...  0.166138  0.004887   \n00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf...  0.181152  0.005814   \n00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a...  0.168335  0.009483   \n\n                                                         B_3     D_42  ...  \\\ncustomer_ID                                                            ...   \n00000469ba478561f23a92a868bd366de6f6527a684c9a2...  0.003576  0.10376  ...   \n00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397...  0.011383      NaN  ...   \n0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5...  0.015945      NaN  ...   \n00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf...  0.498535      NaN  ...   \n00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a...  0.831055      NaN  ...   \n\n                                                    D_136  D_137  D_138  \\\ncustomer_ID                                                               \n00000469ba478561f23a92a868bd366de6f6527a684c9a2...    NaN    NaN    NaN   \n00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397...    NaN    NaN    NaN   \n0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5...    NaN    NaN    NaN   \n00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf...    NaN    NaN    NaN   \n00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a...    NaN    NaN    NaN   \n\n                                                       D_139     D_140  \\\ncustomer_ID                                                              \n00000469ba478561f23a92a868bd366de6f6527a684c9a2...  0.005913  0.001250   \n00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397...  0.004345  0.000866   \n0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5...  1.000977  0.008896   \n00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf...  1.007812  0.003754   \n00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a...  0.006622  0.001140   \n\n                                                       D_141     D_142  \\\ncustomer_ID                                                              \n00000469ba478561f23a92a868bd366de6f6527a684c9a2...  0.006542       NaN   \n00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397...  0.009117       NaN   \n0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5...  0.895996  0.150146   \n00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf...  0.919922  0.255371   \n00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a...  0.009529       NaN   \n\n                                                       D_143     D_144  \\\ncustomer_ID                                                              \n00000469ba478561f23a92a868bd366de6f6527a684c9a2...  0.009163  0.003691   \n00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397...  0.002197  0.000247   \n0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5...  1.009766  0.457764   \n00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf...  1.007812  0.500977   \n00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a...  0.009407  0.001557   \n\n                                                       D_145  \ncustomer_ID                                                   \n00000469ba478561f23a92a868bd366de6f6527a684c9a2...  0.003220  \n00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397...  0.007778  \n0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5...  0.092041  \n00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf...  0.182983  \n00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a...  0.000525  \n\n[5 rows x 189 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>S_2</th>\n      <th>P_2</th>\n      <th>D_39</th>\n      <th>B_1</th>\n      <th>B_2</th>\n      <th>R_1</th>\n      <th>S_3</th>\n      <th>D_41</th>\n      <th>B_3</th>\n      <th>D_42</th>\n      <th>...</th>\n      <th>D_136</th>\n      <th>D_137</th>\n      <th>D_138</th>\n      <th>D_139</th>\n      <th>D_140</th>\n      <th>D_141</th>\n      <th>D_142</th>\n      <th>D_143</th>\n      <th>D_144</th>\n      <th>D_145</th>\n    </tr>\n    <tr>\n      <th>customer_ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7</th>\n      <td>2019-10-12</td>\n      <td>0.568848</td>\n      <td>0.121399</td>\n      <td>0.010780</td>\n      <td>1.009766</td>\n      <td>0.006924</td>\n      <td>0.149414</td>\n      <td>0.000396</td>\n      <td>0.003576</td>\n      <td>0.10376</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.005913</td>\n      <td>0.001250</td>\n      <td>0.006542</td>\n      <td>NaN</td>\n      <td>0.009163</td>\n      <td>0.003691</td>\n      <td>0.003220</td>\n    </tr>\n    <tr>\n      <th>00001bf2e77ff879fab36aa4fac689b9ba411dae63ae397d4263dafa1daedef5</th>\n      <td>2019-04-15</td>\n      <td>0.841309</td>\n      <td>0.126465</td>\n      <td>0.016556</td>\n      <td>1.008789</td>\n      <td>0.009712</td>\n      <td>0.112183</td>\n      <td>0.006191</td>\n      <td>0.011383</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.004345</td>\n      <td>0.000866</td>\n      <td>0.009117</td>\n      <td>NaN</td>\n      <td>0.002197</td>\n      <td>0.000247</td>\n      <td>0.007778</td>\n    </tr>\n    <tr>\n      <th>0000210045da4f81e5f122c6bde5c2a617d03eef67f82c5e400fc98e7bd43ce8</th>\n      <td>2019-10-16</td>\n      <td>0.697754</td>\n      <td>0.002724</td>\n      <td>0.001485</td>\n      <td>0.810059</td>\n      <td>0.002621</td>\n      <td>0.166138</td>\n      <td>0.004887</td>\n      <td>0.015945</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000977</td>\n      <td>0.008896</td>\n      <td>0.895996</td>\n      <td>0.150146</td>\n      <td>1.009766</td>\n      <td>0.457764</td>\n      <td>0.092041</td>\n    </tr>\n    <tr>\n      <th>00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976cf6e56734528702d694</th>\n      <td>2019-04-22</td>\n      <td>0.513184</td>\n      <td>0.324707</td>\n      <td>0.149536</td>\n      <td>0.205688</td>\n      <td>0.002277</td>\n      <td>0.181152</td>\n      <td>0.005814</td>\n      <td>0.498535</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.007812</td>\n      <td>0.003754</td>\n      <td>0.919922</td>\n      <td>0.255371</td>\n      <td>1.007812</td>\n      <td>0.500977</td>\n      <td>0.182983</td>\n    </tr>\n    <tr>\n      <th>00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9a4693dd914fca22557</th>\n      <td>2019-10-22</td>\n      <td>0.254395</td>\n      <td>0.768066</td>\n      <td>0.563477</td>\n      <td>0.038025</td>\n      <td>0.502930</td>\n      <td>0.168335</td>\n      <td>0.009483</td>\n      <td>0.831055</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.006622</td>\n      <td>0.001140</td>\n      <td>0.009529</td>\n      <td>NaN</td>\n      <td>0.009407</td>\n      <td>0.001557</td>\n      <td>0.000525</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 189 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"NaN_Val2 = np.array(test_dataset.isnull().sum())\nNaN_prec2 = np.array((test_dataset.isnull().sum() * 100 / len(test_dataset)).round(2))\nNaN_Col2 = pd.DataFrame([np.array(list(test_dataset.columns)).T,NaN_Val2.T,NaN_prec2.T,np.array(list(test_dataset.dtypes)).T], index=['Features','Num of Missing values','Percentage','DataType']\n).transpose()\npd.set_option('display.max_rows', None)\n\nNaN_Col2","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:41:52.675137Z","iopub.execute_input":"2024-05-22T14:41:52.676351Z","iopub.status.idle":"2024-05-22T14:41:54.304177Z","shell.execute_reply.started":"2024-05-22T14:41:52.676304Z","shell.execute_reply":"2024-05-22T14:41:54.303030Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"    Features Num of Missing values Percentage        DataType\n0        S_2                     0        0.0  datetime64[ns]\n1        P_2                  4784       0.52         float16\n2       D_39                     0        0.0         float16\n3        B_1                     0        0.0         float16\n4        B_2                    43        0.0         float16\n5        R_1                     0        0.0         float16\n6        S_3                144280       15.6         float16\n7       D_41                    43        0.0         float16\n8        B_3                    43        0.0         float16\n9       D_42                827578       89.5         float16\n10      D_43                272218      29.44         float16\n11      D_44                 44612       4.82         float16\n12       B_4                     0        0.0         float16\n13      D_45                    45        0.0         float16\n14       B_5                     0        0.0         float16\n15       R_2                     0        0.0         float16\n16      D_46                179260      19.39         float16\n17      D_47                     0        0.0         float16\n18      D_48                116986      12.65         float16\n19      D_49                802639      86.81         float16\n20       B_6                   138       0.01         float16\n21       B_7                     0        0.0         float16\n22       B_8                 12384       1.34         float16\n23      D_50                529868      57.31         float16\n24      D_51                     0        0.0         float16\n25       B_9                     0        0.0         float16\n26       R_3                     0        0.0         float16\n27      D_52                  2209       0.24         float16\n28       P_3                 30460       3.29         float16\n29      B_10                     0        0.0         float16\n30      D_53                661023      71.49         float16\n31       S_5                     0        0.0         float16\n32      B_11                     0        0.0         float16\n33       S_6                     0        0.0         float16\n34      D_54                    43        0.0         float16\n35       R_4                     0        0.0         float16\n36       S_7                144280       15.6         float16\n37      B_12                     0        0.0         float16\n38       S_8                     0        0.0         float16\n39      D_55                 60438       6.54         float16\n40      D_56                463223       50.1         float16\n41      B_13                  2010       0.22         float16\n42       R_5                     0        0.0         float16\n43      D_58                     0        0.0         float16\n44       S_9                200608       21.7         float16\n45      B_14                     0        0.0         float16\n46      D_59                  8991       0.97         float16\n47      D_60                     0        0.0         float16\n48      D_61                101585      10.99         float16\n49      B_15                  1409       0.15         float16\n50      S_11                     0        0.0         float16\n51      D_62                154713      16.73         float16\n52      D_63                     0        0.0        category\n53      D_64                     0        0.0        category\n54      D_65                     0        0.0         float16\n55      B_16                    43        0.0         float16\n56      B_17                482036      52.13         float16\n57      B_18                     0        0.0         float16\n58      B_19                    43        0.0         float16\n59      D_66                813168      87.95        category\n60      B_20                    43        0.0         float16\n61      D_68                 22380       2.42        category\n62      S_12                   611       0.07         float16\n63       R_6                     0        0.0         float16\n64      S_13                     0        0.0         float16\n65      B_21                     0        0.0         float16\n66      D_69                 17657       1.91         float16\n67      B_22                    43        0.0         float16\n68      D_70                  9205        1.0         float16\n69      D_71                     0        0.0         float16\n70      D_72                  2064       0.22         float16\n71      S_15                     0        0.0         float16\n72      B_23                     0        0.0         float16\n73      D_73                910964      98.52         float16\n74       P_4                     0        0.0         float16\n75      D_74                  4179       0.45         float16\n76      D_75                     0        0.0         float16\n77      D_76                831348      89.91         float16\n78      B_24                     0        0.0         float16\n79       R_7                     0        0.0         float16\n80      D_77                464536      50.24         float16\n81      B_25                  1409       0.15         float16\n82      B_26                    43        0.0         float16\n83      D_78                 44612       4.82         float16\n84      D_79                  4720       0.51         float16\n85       R_8                     0        0.0         float16\n86       R_9                874673       94.6         float16\n87      S_16                     0        0.0         float16\n88      D_80                  4179       0.45         float16\n89      R_10                     0        0.0         float16\n90      R_11                     0        0.0         float16\n91      B_27                    43        0.0         float16\n92      D_81                  2087       0.23         float16\n93      D_82                689095      74.53         float16\n94      S_17                  1409       0.15         float16\n95      R_12                     0        0.0         float16\n96      B_28                     0        0.0         float16\n97      R_13                     0        0.0         float16\n98      D_83                 17657       1.91         float16\n99      R_14                     0        0.0         float16\n100     R_15                     0        0.0         float16\n101     D_84                  2209       0.24         float16\n102     R_16                     0        0.0         float16\n103     B_29                436687      47.23         float16\n104     B_30                    43        0.0        category\n105     S_18                     0        0.0         float16\n106     D_86                   122       0.01         float16\n107     D_87                923023      99.83         float16\n108     R_17                     0        0.0         float16\n109     R_18                     0        0.0         float16\n110     D_88                923072      99.83         float16\n111     B_31                     0        0.0         float16\n112     S_19                     0        0.0         float16\n113     R_19                     0        0.0         float16\n114     B_32                     0        0.0         float16\n115     S_20                     0        0.0         float16\n116     R_20                     0        0.0         float16\n117     R_21                     0        0.0         float16\n118     B_33                    43        0.0         float16\n119     D_89                  2209       0.24         float16\n120     R_22                     0        0.0         float16\n121     R_23                     0        0.0         float16\n122     D_91                 34513       3.73         float16\n123     D_92                     0        0.0         float16\n124     D_93                     0        0.0         float16\n125     D_94                     0        0.0         float16\n126     R_24                     0        0.0         float16\n127     R_25                     0        0.0         float16\n128     D_96                     0        0.0         float16\n129     S_22                  4714       0.51         float16\n130     S_23                   157       0.02         float16\n131     S_24                  4634        0.5         float16\n132     S_25                  2953       0.32         float16\n133     S_26                   834       0.09         float16\n134    D_102                     0        0.0         float16\n135    D_103                  5050       0.55         float16\n136    D_104                  5050       0.55         float16\n137    D_105                490747      53.08         float16\n138    D_106                803135      86.86         float16\n139    D_107                  5050       0.55         float16\n140     B_36                     0        0.0         float16\n141     B_37                   834       0.09         float16\n142     R_26                775847      83.91         float16\n143     R_27                 51391       5.56         float16\n144     B_38                    43        0.0        category\n145    D_108                920135      99.51         float16\n146    D_109                   877       0.09         float16\n147    D_110                914345      98.89         float16\n148    D_111                914345      98.89         float16\n149     B_39                912962      98.74         float16\n150    D_112                   877       0.09         float16\n151     B_40                   834       0.09         float16\n152     S_27                208690      22.57         float16\n153    D_113                 17018       1.84         float16\n154    D_114                 17018       1.84        category\n155    D_115                 17018       1.84         float16\n156    D_116                 17018       1.84        category\n157    D_117                 17018       1.84        category\n158    D_118                 17018       1.84         float16\n159    D_119                 17018       1.84         float16\n160    D_120                 17018       1.84        category\n161    D_121                 17018       1.84         float16\n162    D_122                 17018       1.84         float16\n163    D_123                 17018       1.84         float16\n164    D_124                 17018       1.84         float16\n165    D_125                 17018       1.84         float16\n166    D_126                     0        0.0        category\n167    D_127                     0        0.0         float16\n168    D_128                  5050       0.55         float16\n169    D_129                  5050       0.55         float16\n170     B_41                   834       0.09         float16\n171     B_42                906446      98.03         float16\n172    D_130                  5050       0.55         float16\n173    D_131                  5050       0.55         float16\n174    D_132                803056      86.85         float16\n175    D_133                     0        0.0         float16\n176     R_28                     0        0.0         float16\n177    D_134                897699      97.09         float16\n178    D_135                897699      97.09         float16\n179    D_136                897699      97.09         float16\n180    D_137                897699      97.09         float16\n181    D_138                897699      97.09         float16\n182    D_139                  5050       0.55         float16\n183    D_140                     0        0.0         float16\n184    D_141                  5050       0.55         float16\n185    D_142                763770       82.6         float16\n186    D_143                  5050       0.55         float16\n187    D_144                     0        0.0         float16\n188    D_145                  5050       0.55         float16","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Features</th>\n      <th>Num of Missing values</th>\n      <th>Percentage</th>\n      <th>DataType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>S_2</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>datetime64[ns]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P_2</td>\n      <td>4784</td>\n      <td>0.52</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>D_39</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B_1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B_2</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>R_1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>S_3</td>\n      <td>144280</td>\n      <td>15.6</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>D_41</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>B_3</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>D_42</td>\n      <td>827578</td>\n      <td>89.5</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>D_43</td>\n      <td>272218</td>\n      <td>29.44</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>D_44</td>\n      <td>44612</td>\n      <td>4.82</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>B_4</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>D_45</td>\n      <td>45</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>B_5</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>R_2</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>D_46</td>\n      <td>179260</td>\n      <td>19.39</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>D_47</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>D_48</td>\n      <td>116986</td>\n      <td>12.65</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>D_49</td>\n      <td>802639</td>\n      <td>86.81</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>B_6</td>\n      <td>138</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>B_7</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>B_8</td>\n      <td>12384</td>\n      <td>1.34</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>D_50</td>\n      <td>529868</td>\n      <td>57.31</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>D_51</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>B_9</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>R_3</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>D_52</td>\n      <td>2209</td>\n      <td>0.24</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>P_3</td>\n      <td>30460</td>\n      <td>3.29</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>B_10</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>D_53</td>\n      <td>661023</td>\n      <td>71.49</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>S_5</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>B_11</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>S_6</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>D_54</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>R_4</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>S_7</td>\n      <td>144280</td>\n      <td>15.6</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>B_12</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>S_8</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>D_55</td>\n      <td>60438</td>\n      <td>6.54</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>D_56</td>\n      <td>463223</td>\n      <td>50.1</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>B_13</td>\n      <td>2010</td>\n      <td>0.22</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>R_5</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>D_58</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>S_9</td>\n      <td>200608</td>\n      <td>21.7</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>B_14</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>D_59</td>\n      <td>8991</td>\n      <td>0.97</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>D_60</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>D_61</td>\n      <td>101585</td>\n      <td>10.99</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>B_15</td>\n      <td>1409</td>\n      <td>0.15</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>S_11</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>D_62</td>\n      <td>154713</td>\n      <td>16.73</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>D_63</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>D_64</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>D_65</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>B_16</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>B_17</td>\n      <td>482036</td>\n      <td>52.13</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>B_18</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>B_19</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>D_66</td>\n      <td>813168</td>\n      <td>87.95</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>B_20</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>D_68</td>\n      <td>22380</td>\n      <td>2.42</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>S_12</td>\n      <td>611</td>\n      <td>0.07</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>R_6</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>S_13</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>B_21</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>D_69</td>\n      <td>17657</td>\n      <td>1.91</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>B_22</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>D_70</td>\n      <td>9205</td>\n      <td>1.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>D_71</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>D_72</td>\n      <td>2064</td>\n      <td>0.22</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>S_15</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>B_23</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>D_73</td>\n      <td>910964</td>\n      <td>98.52</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>P_4</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>D_74</td>\n      <td>4179</td>\n      <td>0.45</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>D_75</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>D_76</td>\n      <td>831348</td>\n      <td>89.91</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>B_24</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>R_7</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>D_77</td>\n      <td>464536</td>\n      <td>50.24</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>B_25</td>\n      <td>1409</td>\n      <td>0.15</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>B_26</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>D_78</td>\n      <td>44612</td>\n      <td>4.82</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>D_79</td>\n      <td>4720</td>\n      <td>0.51</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>R_8</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>R_9</td>\n      <td>874673</td>\n      <td>94.6</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>S_16</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>D_80</td>\n      <td>4179</td>\n      <td>0.45</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>R_10</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>R_11</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>B_27</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>D_81</td>\n      <td>2087</td>\n      <td>0.23</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>D_82</td>\n      <td>689095</td>\n      <td>74.53</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>S_17</td>\n      <td>1409</td>\n      <td>0.15</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>R_12</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>B_28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>R_13</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>D_83</td>\n      <td>17657</td>\n      <td>1.91</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>R_14</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>R_15</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>D_84</td>\n      <td>2209</td>\n      <td>0.24</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>R_16</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>B_29</td>\n      <td>436687</td>\n      <td>47.23</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>B_30</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>S_18</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>D_86</td>\n      <td>122</td>\n      <td>0.01</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>D_87</td>\n      <td>923023</td>\n      <td>99.83</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>R_17</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>R_18</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>D_88</td>\n      <td>923072</td>\n      <td>99.83</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>B_31</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>S_19</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>R_19</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>B_32</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>S_20</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>R_20</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>R_21</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>B_33</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>D_89</td>\n      <td>2209</td>\n      <td>0.24</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>R_22</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>R_23</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>D_91</td>\n      <td>34513</td>\n      <td>3.73</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>D_92</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>D_93</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>D_94</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>R_24</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>R_25</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>D_96</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>S_22</td>\n      <td>4714</td>\n      <td>0.51</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>S_23</td>\n      <td>157</td>\n      <td>0.02</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>S_24</td>\n      <td>4634</td>\n      <td>0.5</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>S_25</td>\n      <td>2953</td>\n      <td>0.32</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>S_26</td>\n      <td>834</td>\n      <td>0.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>D_102</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>D_103</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>D_104</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>D_105</td>\n      <td>490747</td>\n      <td>53.08</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>D_106</td>\n      <td>803135</td>\n      <td>86.86</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>D_107</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>B_36</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>B_37</td>\n      <td>834</td>\n      <td>0.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>R_26</td>\n      <td>775847</td>\n      <td>83.91</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>R_27</td>\n      <td>51391</td>\n      <td>5.56</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>B_38</td>\n      <td>43</td>\n      <td>0.0</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>D_108</td>\n      <td>920135</td>\n      <td>99.51</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>D_109</td>\n      <td>877</td>\n      <td>0.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>D_110</td>\n      <td>914345</td>\n      <td>98.89</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>D_111</td>\n      <td>914345</td>\n      <td>98.89</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>B_39</td>\n      <td>912962</td>\n      <td>98.74</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>D_112</td>\n      <td>877</td>\n      <td>0.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>B_40</td>\n      <td>834</td>\n      <td>0.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>152</th>\n      <td>S_27</td>\n      <td>208690</td>\n      <td>22.57</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>D_113</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>D_114</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>155</th>\n      <td>D_115</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>D_116</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>157</th>\n      <td>D_117</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>158</th>\n      <td>D_118</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>159</th>\n      <td>D_119</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>160</th>\n      <td>D_120</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>D_121</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>162</th>\n      <td>D_122</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>163</th>\n      <td>D_123</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>164</th>\n      <td>D_124</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>D_125</td>\n      <td>17018</td>\n      <td>1.84</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>166</th>\n      <td>D_126</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>category</td>\n    </tr>\n    <tr>\n      <th>167</th>\n      <td>D_127</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>168</th>\n      <td>D_128</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>169</th>\n      <td>D_129</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>B_41</td>\n      <td>834</td>\n      <td>0.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>B_42</td>\n      <td>906446</td>\n      <td>98.03</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>D_130</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>D_131</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>D_132</td>\n      <td>803056</td>\n      <td>86.85</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>D_133</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>R_28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>D_134</td>\n      <td>897699</td>\n      <td>97.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>D_135</td>\n      <td>897699</td>\n      <td>97.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>D_136</td>\n      <td>897699</td>\n      <td>97.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>D_137</td>\n      <td>897699</td>\n      <td>97.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>D_138</td>\n      <td>897699</td>\n      <td>97.09</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>182</th>\n      <td>D_139</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>D_140</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>D_141</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>D_142</td>\n      <td>763770</td>\n      <td>82.6</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>D_143</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>D_144</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>float16</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>D_145</td>\n      <td>5050</td>\n      <td>0.55</td>\n      <td>float16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_dataset = test_dataset.drop(['S_2','D_42','D_49','D_66','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:42:06.132812Z","iopub.execute_input":"2024-05-22T14:42:06.133218Z","iopub.status.idle":"2024-05-22T14:42:06.669847Z","shell.execute_reply.started":"2024-05-22T14:42:06.133183Z","shell.execute_reply":"2024-05-22T14:42:06.668644Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"selected_column = np.array(['P_2','S_3','B_2','D_41','D_43','B_3','D_44','D_45','D_46','D_48','D_50','D_53','S_7','D_56','S_9','S_12','S_17','B_6','B_8','D_52','P_3','D_54','D_55','B_13','D_59','D_61','B_15','D_62','B_16','B_17','D_77','B_19','B_20','D_69','B_22','D_70','D_72','D_74','R_7','B_25','B_26','D_78','D_79','D_80','B_27','D_81','R_12','D_82','D_105','S_27','D_83','R_14','D_84','D_86','R_20','B_33','D_89','D_91','S_22','S_23','S_24','S_25','S_26','D_102','D_103','D_104','D_107','B_37','R_27','D_109','D_112','B_40','D_113','D_115','D_118','D_119','D_121','D_122','D_123','D_124','D_125','D_128','D_129','B_41','D_130','D_131','D_133','D_139','D_140','D_141','D_143','D_144','D_145'])\n\nfor column in selected_column:\n    test_dataset[column] = test_dataset[column].fillna(test_dataset[column].median())","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:42:12.830581Z","iopub.execute_input":"2024-05-22T14:42:12.831007Z","iopub.status.idle":"2024-05-22T14:42:17.212820Z","shell.execute_reply.started":"2024-05-22T14:42:12.830974Z","shell.execute_reply":"2024-05-22T14:42:17.211607Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"selected_column2 = np.array(['D_68','B_30','B_38','D_114','D_116','D_117','D_120','D_126'])\n\nfor column2 in selected_column2:\n    test_dataset[column2] =  test_dataset[column2].fillna(test_dataset[column2].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:42:19.635770Z","iopub.execute_input":"2024-05-22T14:42:19.636155Z","iopub.status.idle":"2024-05-22T14:42:19.704450Z","shell.execute_reply.started":"2024-05-22T14:42:19.636126Z","shell.execute_reply":"2024-05-22T14:42:19.703068Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(test_dataset.isnull().sum().to_string())","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:42:28.453947Z","iopub.execute_input":"2024-05-22T14:42:28.454408Z","iopub.status.idle":"2024-05-22T14:42:29.307728Z","shell.execute_reply.started":"2024-05-22T14:42:28.454373Z","shell.execute_reply":"2024-05-22T14:42:29.306666Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"P_2      0\nD_39     0\nB_1      0\nB_2      0\nR_1      0\nS_3      0\nD_41     0\nB_3      0\nD_43     0\nD_44     0\nB_4      0\nD_45     0\nB_5      0\nR_2      0\nD_46     0\nD_47     0\nD_48     0\nB_6      0\nB_7      0\nB_8      0\nD_50     0\nD_51     0\nB_9      0\nR_3      0\nD_52     0\nP_3      0\nB_10     0\nD_53     0\nS_5      0\nB_11     0\nS_6      0\nD_54     0\nR_4      0\nS_7      0\nB_12     0\nS_8      0\nD_55     0\nD_56     0\nB_13     0\nR_5      0\nD_58     0\nS_9      0\nB_14     0\nD_59     0\nD_60     0\nD_61     0\nB_15     0\nS_11     0\nD_62     0\nD_63     0\nD_64     0\nD_65     0\nB_16     0\nB_17     0\nB_18     0\nB_19     0\nB_20     0\nD_68     0\nS_12     0\nR_6      0\nS_13     0\nB_21     0\nD_69     0\nB_22     0\nD_70     0\nD_71     0\nD_72     0\nS_15     0\nB_23     0\nP_4      0\nD_74     0\nD_75     0\nB_24     0\nR_7      0\nD_77     0\nB_25     0\nB_26     0\nD_78     0\nD_79     0\nR_8      0\nS_16     0\nD_80     0\nR_10     0\nR_11     0\nB_27     0\nD_81     0\nD_82     0\nS_17     0\nR_12     0\nB_28     0\nR_13     0\nD_83     0\nR_14     0\nR_15     0\nD_84     0\nR_16     0\nB_30     0\nS_18     0\nD_86     0\nR_17     0\nR_18     0\nB_31     0\nS_19     0\nR_19     0\nB_32     0\nS_20     0\nR_20     0\nR_21     0\nB_33     0\nD_89     0\nR_22     0\nR_23     0\nD_91     0\nD_92     0\nD_93     0\nD_94     0\nR_24     0\nR_25     0\nD_96     0\nS_22     0\nS_23     0\nS_24     0\nS_25     0\nS_26     0\nD_102    0\nD_103    0\nD_104    0\nD_105    0\nD_107    0\nB_36     0\nB_37     0\nR_27     0\nB_38     0\nD_109    0\nD_112    0\nB_40     0\nS_27     0\nD_113    0\nD_114    0\nD_115    0\nD_116    0\nD_117    0\nD_118    0\nD_119    0\nD_120    0\nD_121    0\nD_122    0\nD_123    0\nD_124    0\nD_125    0\nD_126    0\nD_127    0\nD_128    0\nD_129    0\nB_41     0\nD_130    0\nD_131    0\nD_133    0\nR_28     0\nD_139    0\nD_140    0\nD_141    0\nD_143    0\nD_144    0\nD_145    0\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_dataset.isnull().sum().to_string())","metadata":{"execution":{"iopub.status.busy":"2024-02-14T23:57:30.896156Z","iopub.execute_input":"2024-02-14T23:57:30.896547Z","iopub.status.idle":"2024-02-14T23:57:31.058773Z","shell.execute_reply.started":"2024-02-14T23:57:30.896515Z","shell.execute_reply":"2024-02-14T23:57:31.057741Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"P_2       0\nD_39      0\nB_1       0\nB_2       0\nR_1       0\nS_3       0\nD_41      0\nB_3       0\nD_43      0\nD_44      0\nB_4       0\nD_45      0\nB_5       0\nR_2       0\nD_46      0\nD_47      0\nD_48      0\nB_6       0\nB_7       0\nB_8       0\nD_50      0\nD_51      0\nB_9       0\nR_3       0\nD_52      0\nP_3       0\nB_10      0\nD_53      0\nS_5       0\nB_11      0\nS_6       0\nD_54      0\nR_4       0\nS_7       0\nB_12      0\nS_8       0\nD_55      0\nD_56      0\nB_13      0\nR_5       0\nD_58      0\nS_9       0\nB_14      0\nD_59      0\nD_60      0\nD_61      0\nB_15      0\nS_11      0\nD_62      0\nD_63      0\nD_64      0\nD_65      0\nB_16      0\nB_17      0\nB_18      0\nB_19      0\nB_20      0\nD_68      0\nS_12      0\nR_6       0\nS_13      0\nB_21      0\nD_69      0\nB_22      0\nD_70      0\nD_71      0\nD_72      0\nS_15      0\nB_23      0\nP_4       0\nD_74      0\nD_75      0\nB_24      0\nR_7       0\nD_77      0\nB_25      0\nB_26      0\nD_78      0\nD_79      0\nR_8       0\nS_16      0\nD_80      0\nR_10      0\nR_11      0\nB_27      0\nD_81      0\nD_82      0\nS_17      0\nR_12      0\nB_28      0\nR_13      0\nD_83      0\nR_14      0\nR_15      0\nD_84      0\nR_16      0\nB_30      0\nS_18      0\nD_86      0\nR_17      0\nR_18      0\nB_31      0\nS_19      0\nR_19      0\nB_32      0\nS_20      0\nR_20      0\nR_21      0\nB_33      0\nD_89      0\nR_22      0\nR_23      0\nD_91      0\nD_92      0\nD_93      0\nD_94      0\nR_24      0\nR_25      0\nD_96      0\nS_22      0\nS_23      0\nS_24      0\nS_25      0\nS_26      0\nD_102     0\nD_103     0\nD_104     0\nD_105     0\nD_107     0\nB_36      0\nB_37      0\nR_27      0\nB_38      0\nD_109     0\nD_112     0\nB_40      0\nS_27      0\nD_113     0\nD_114     0\nD_115     0\nD_116     0\nD_117     0\nD_118     0\nD_119     0\nD_120     0\nD_121     0\nD_122     0\nD_123     0\nD_124     0\nD_125     0\nD_126     0\nD_127     0\nD_128     0\nD_129     0\nB_41      0\nD_130     0\nD_131     0\nD_133     0\nR_28      0\nD_139     0\nD_140     0\nD_141     0\nD_143     0\nD_144     0\nD_145     0\ntarget    0\n","output_type":"stream"}]},{"cell_type":"code","source":"enc = OrdinalEncoder()\ncategorical_cols.remove('D_66')\n\ntrain_dataset[categorical_cols] = enc.fit_transform(train_dataset[categorical_cols])\ntest_dataset[categorical_cols] = enc.transform(test_dataset[categorical_cols])","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:50:29.638717Z","iopub.execute_input":"2024-05-22T14:50:29.639167Z","iopub.status.idle":"2024-05-22T14:50:32.436228Z","shell.execute_reply.started":"2024-05-22T14:50:29.639126Z","shell.execute_reply":"2024-05-22T14:50:32.434861Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_dataset_without_target = train_dataset.drop([\"target\"],axis=1)\n\ncor_matrix = train_dataset_without_target.corr()\ncol_core = set()\n\nfor i in range(len(cor_matrix.columns)):\n    for j in range(i):\n        if(cor_matrix.iloc[i, j] > 0.9):\n            col_name = cor_matrix.columns[i]\n            col_core.add(col_name)\ncol_core","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:50:51.409605Z","iopub.execute_input":"2024-05-22T14:50:51.410145Z","iopub.status.idle":"2024-05-22T14:51:27.029185Z","shell.execute_reply.started":"2024-05-22T14:50:51.410103Z","shell.execute_reply":"2024-05-22T14:51:27.028089Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'B_11',\n 'B_13',\n 'B_15',\n 'B_23',\n 'B_33',\n 'B_37',\n 'D_104',\n 'D_119',\n 'D_141',\n 'D_143',\n 'D_74',\n 'D_75',\n 'D_77',\n 'S_24',\n 'S_7'}"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = train_dataset.drop(col_core, axis=1)\ntest_dataset = test_dataset.drop(col_core, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T14:53:20.042703Z","iopub.execute_input":"2024-05-22T14:53:20.043188Z","iopub.status.idle":"2024-05-22T14:53:21.597206Z","shell.execute_reply.started":"2024-05-22T14:53:20.043153Z","shell.execute_reply":"2024-05-22T14:53:21.595844Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"num_columns = [col for col in train_dataset.columns if col not in [\"target\"]]\n\nX = train_dataset[num_columns]\ny = train_dataset['target']\n\nprint(f\"X shape is = {X.shape}\" )\nprint(f\"Y shape is = {y.shape}\" )","metadata":{"execution":{"iopub.status.busy":"2024-05-22T15:14:21.902665Z","iopub.execute_input":"2024-05-22T15:14:21.903190Z","iopub.status.idle":"2024-05-22T15:14:22.346394Z","shell.execute_reply.started":"2024-05-22T15:14:21.903156Z","shell.execute_reply":"2024-05-22T15:14:22.344676Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"X shape is = (458913, 150)\nY shape is = (458913,)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train = X\nY_train = y","metadata":{"execution":{"iopub.status.busy":"2024-05-22T15:19:15.007866Z","iopub.execute_input":"2024-05-22T15:19:15.008292Z","iopub.status.idle":"2024-05-22T15:19:15.013633Z","shell.execute_reply.started":"2024-05-22T15:19:15.008249Z","shell.execute_reply":"2024-05-22T15:19:15.012356Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"X_train shape is = {x_train.shape}\" )\nprint(f\"Y_train shape is = {y_train.shape}\" )\nprint(f\"X_test shape is = {x_test.shape}\" )\nprint(f\"Y_test shape is = {y_test.shape}\" )","metadata":{"execution":{"iopub.status.busy":"2024-05-22T15:19:32.392382Z","iopub.execute_input":"2024-05-22T15:19:32.392954Z","iopub.status.idle":"2024-05-22T15:19:33.510625Z","shell.execute_reply.started":"2024-05-22T15:19:32.392916Z","shell.execute_reply":"2024-05-22T15:19:33.509335Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"X_train shape is = (367130, 150)\nY_train shape is = (367130,)\nX_test shape is = (91783, 150)\nY_test shape is = (91783,)\n","output_type":"stream"}]},{"cell_type":"code","source":"def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n\n    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n        df = (pd.concat([y_true, y_pred], axis='columns')\n              .sort_values('prediction', ascending=False))\n        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n        four_pct_cutoff = int(0.04 * df['weight'].sum())\n        df['weight_cumsum'] = df['weight'].cumsum()\n        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n        \n    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n        df = (pd.concat([y_true, y_pred], axis='columns')\n              .sort_values('prediction', ascending=False))\n        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n        total_pos = (df['target'] * df['weight']).sum()\n        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n        df['lorentz'] = df['cum_pos_found'] / total_pos\n        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n        return df['gini'].sum()\n\n    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n\n    g = normalized_weighted_gini(y_true, y_pred)\n    d = top_four_percent_captured(y_true, y_pred)\n\n    return 0.5 * (g + d)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T15:20:35.641729Z","iopub.execute_input":"2024-05-22T15:20:35.642185Z","iopub.status.idle":"2024-05-22T15:20:35.661129Z","shell.execute_reply.started":"2024-05-22T15:20:35.642150Z","shell.execute_reply":"2024-05-22T15:20:35.659585Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    n_estimators = trial.suggest_int('n_estimators',1000,5000,200)\n    num_leaves = trial.suggest_int('num_leaves',10,100,10)\n    learning_rate = trial.suggest_uniform('learning_rate',1e-5,1e-2)\n    min_data_in_leaf =  trial.suggest_int('min_data_in_leaf', 5, 100)\n    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.4, 1.0)\n    feature_fraction = trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n    \n    params = {'boosting_type': 'gbdt',\n              'n_estimators': n_estimators,\n              'num_leaves': num_leaves,\n              'learning_rate': learning_rate,\n              'min_data_in_leaf': min_data_in_leaf,\n              'bagging_fraction': bagging_fraction,\n              'feature_fraction': feature_fraction,\n              #'max_bins': 500,\n              #'reg_alpha': 2,\n              'objective': 'binary',\n              'metric':'binary_logloss',\n              'device':'cpu',\n              'random_state': 42}\n    \n    y_valid, gbm_val_probs, gbm_test_preds, gini=[],[],[],[]\n    \n    gbm = LGBMClassifier(**params).fit(x_train, y_train, \n                                       eval_set=[(x_test, y_test)],\n                                       callbacks=[early_stopping(200), log_evaluation(500)],\n                                       eval_metric=['auc','binary_logloss'])\n        \n    predictions = gbm.predict_proba(x_test)[:,1]   \n    \n    y_pred=pd.DataFrame(data={'prediction':predictions})\n    y_true=pd.DataFrame(data={'target':y_test.reset_index(drop=True)})\n    gini_score=amex_metric(y_true = y_true, y_pred = y_pred)\n     \n    #ft_importance[\"Importance_Fold\"+str(fold)]=gbm.feature_importances_    \n    print(\"Validation Gini: {:.6f}\".format(gini_score))\n    \n    \n    _ = gc.collect()\n    \n    \n    \n    return gini_score\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=30)","metadata":{"execution":{"iopub.status.busy":"2024-02-15T00:28:58.309362Z","iopub.execute_input":"2024-02-15T00:28:58.309775Z","iopub.status.idle":"2024-02-15T02:49:59.553010Z","shell.execute_reply.started":"2024-02-15T00:28:58.309742Z","shell.execute_reply":"2024-02-15T02:49:59.550451Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"[I 2024-02-15 00:28:58,317] A new study created in memory with name: no-name-bd0e6394-4eff-492c-89e0-ff4f7f7d9207\n/tmp/ipykernel_33/2812364169.py:2: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  n_estimators = trial.suggest_int('n_estimators',1000,5000,200)\n/tmp/ipykernel_33/2812364169.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  num_leaves = trial.suggest_int('num_leaves',10,100,10)\n/tmp/ipykernel_33/2812364169.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  learning_rate = trial.suggest_uniform('learning_rate',1e-5,1e-2)\n/tmp/ipykernel_33/2812364169.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.4, 1.0)\n/tmp/ipykernel_33/2812364169.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  feature_fraction = trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n[LightGBM] [Warning] feature_fraction is set=0.5296547747766825, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5296547747766825\n[LightGBM] [Warning] bagging_fraction is set=0.4774476139295071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4774476139295071\n[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n[LightGBM] [Warning] feature_fraction is set=0.5296547747766825, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5296547747766825\n[LightGBM] [Warning] bagging_fraction is set=0.4774476139295071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4774476139295071\n[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.818490 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 35440\n[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 150\n[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n[LightGBM] [Warning] feature_fraction is set=0.5296547747766825, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5296547747766825\n[LightGBM] [Warning] bagging_fraction is set=0.4774476139295071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4774476139295071\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n[LightGBM] [Info] Start training from score -1.051523\nTraining until validation scores don't improve for 200 rounds\n[500]\tvalid_0's auc: 0.956218\tvalid_0's binary_logloss: 0.24439\n[1000]\tvalid_0's auc: 0.958913\tvalid_0's binary_logloss: 0.226512\n[1500]\tvalid_0's auc: 0.960055\tvalid_0's binary_logloss: 0.222465\nDid not meet early stopping. Best iteration is:\n[1600]\tvalid_0's auc: 0.96018\tvalid_0's binary_logloss: 0.222058\n[LightGBM] [Warning] min_data_in_leaf is set=79, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=79\n[LightGBM] [Warning] feature_fraction is set=0.5296547747766825, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5296547747766825\n[LightGBM] [Warning] bagging_fraction is set=0.4774476139295071, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4774476139295071\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-02-15 00:42:24,985] Trial 0 finished with value: 0.7876853350072206 and parameters: {'n_estimators': 1600, 'num_leaves': 60, 'learning_rate': 0.005700454333752762, 'min_data_in_leaf': 79, 'bagging_fraction': 0.4774476139295071, 'feature_fraction': 0.5296547747766825}. Best is trial 0 with value: 0.7876853350072206.\n","output_type":"stream"},{"name":"stdout","text":"Validation Gini: 0.787685\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/2812364169.py:2: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  n_estimators = trial.suggest_int('n_estimators',1000,5000,200)\n/tmp/ipykernel_33/2812364169.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  num_leaves = trial.suggest_int('num_leaves',10,100,10)\n/tmp/ipykernel_33/2812364169.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  learning_rate = trial.suggest_uniform('learning_rate',1e-5,1e-2)\n/tmp/ipykernel_33/2812364169.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.4, 1.0)\n/tmp/ipykernel_33/2812364169.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  feature_fraction = trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n[LightGBM] [Warning] feature_fraction is set=0.8576917706000423, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8576917706000423\n[LightGBM] [Warning] bagging_fraction is set=0.9564126735455002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9564126735455002\n[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n[LightGBM] [Warning] feature_fraction is set=0.8576917706000423, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8576917706000423\n[LightGBM] [Warning] bagging_fraction is set=0.9564126735455002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9564126735455002\n[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.050579 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 35440\n[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 150\n[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n[LightGBM] [Warning] feature_fraction is set=0.8576917706000423, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8576917706000423\n[LightGBM] [Warning] bagging_fraction is set=0.9564126735455002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9564126735455002\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n[LightGBM] [Info] Start training from score -1.051523\nTraining until validation scores don't improve for 200 rounds\n[500]\tvalid_0's auc: 0.958343\tvalid_0's binary_logloss: 0.228675\n[1000]\tvalid_0's auc: 0.960183\tvalid_0's binary_logloss: 0.221814\n[1500]\tvalid_0's auc: 0.960485\tvalid_0's binary_logloss: 0.22083\n[2000]\tvalid_0's auc: 0.960518\tvalid_0's binary_logloss: 0.22067\nEarly stopping, best iteration is:\n[1898]\tvalid_0's auc: 0.96052\tvalid_0's binary_logloss: 0.220678\n[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n[LightGBM] [Warning] feature_fraction is set=0.8576917706000423, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8576917706000423\n[LightGBM] [Warning] bagging_fraction is set=0.9564126735455002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9564126735455002\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-02-15 01:06:56,063] Trial 1 finished with value: 0.7881512757776805 and parameters: {'n_estimators': 3400, 'num_leaves': 100, 'learning_rate': 0.009116643689805394, 'min_data_in_leaf': 34, 'bagging_fraction': 0.9564126735455002, 'feature_fraction': 0.8576917706000423}. Best is trial 1 with value: 0.7881512757776805.\n","output_type":"stream"},{"name":"stdout","text":"Validation Gini: 0.788151\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/2812364169.py:2: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  n_estimators = trial.suggest_int('n_estimators',1000,5000,200)\n/tmp/ipykernel_33/2812364169.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  num_leaves = trial.suggest_int('num_leaves',10,100,10)\n/tmp/ipykernel_33/2812364169.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  learning_rate = trial.suggest_uniform('learning_rate',1e-5,1e-2)\n/tmp/ipykernel_33/2812364169.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.4, 1.0)\n/tmp/ipykernel_33/2812364169.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  feature_fraction = trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n[LightGBM] [Warning] feature_fraction is set=0.7855926604798544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7855926604798544\n[LightGBM] [Warning] bagging_fraction is set=0.9228673064091857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9228673064091857\n[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n[LightGBM] [Warning] feature_fraction is set=0.7855926604798544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7855926604798544\n[LightGBM] [Warning] bagging_fraction is set=0.9228673064091857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9228673064091857\n[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.069552 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 35440\n[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 150\n[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n[LightGBM] [Warning] feature_fraction is set=0.7855926604798544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7855926604798544\n[LightGBM] [Warning] bagging_fraction is set=0.9228673064091857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9228673064091857\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n[LightGBM] [Info] Start training from score -1.051523\nTraining until validation scores don't improve for 200 rounds\n[500]\tvalid_0's auc: 0.95212\tvalid_0's binary_logloss: 0.325222\n[1000]\tvalid_0's auc: 0.954514\tvalid_0's binary_logloss: 0.263161\n[1500]\tvalid_0's auc: 0.95625\tvalid_0's binary_logloss: 0.241548\nDid not meet early stopping. Best iteration is:\n[1800]\tvalid_0's auc: 0.957059\tvalid_0's binary_logloss: 0.235312\n[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n[LightGBM] [Warning] feature_fraction is set=0.7855926604798544, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7855926604798544\n[LightGBM] [Warning] bagging_fraction is set=0.9228673064091857, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9228673064091857\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-02-15 01:31:18,161] Trial 2 finished with value: 0.7744035875909059 and parameters: {'n_estimators': 1800, 'num_leaves': 70, 'learning_rate': 0.001995594280740615, 'min_data_in_leaf': 60, 'bagging_fraction': 0.9228673064091857, 'feature_fraction': 0.7855926604798544}. Best is trial 1 with value: 0.7881512757776805.\n","output_type":"stream"},{"name":"stdout","text":"Validation Gini: 0.774404\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/2812364169.py:2: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  n_estimators = trial.suggest_int('n_estimators',1000,5000,200)\n/tmp/ipykernel_33/2812364169.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  num_leaves = trial.suggest_int('num_leaves',10,100,10)\n/tmp/ipykernel_33/2812364169.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  learning_rate = trial.suggest_uniform('learning_rate',1e-5,1e-2)\n/tmp/ipykernel_33/2812364169.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.4, 1.0)\n/tmp/ipykernel_33/2812364169.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  feature_fraction = trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Warning] feature_fraction is set=0.9414629297470957, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9414629297470957\n[LightGBM] [Warning] bagging_fraction is set=0.9077035865260106, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9077035865260106\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Warning] feature_fraction is set=0.9414629297470957, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9414629297470957\n[LightGBM] [Warning] bagging_fraction is set=0.9077035865260106, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9077035865260106\n[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.146522 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 35440\n[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 150\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Warning] feature_fraction is set=0.9414629297470957, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9414629297470957\n[LightGBM] [Warning] bagging_fraction is set=0.9077035865260106, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9077035865260106\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n[LightGBM] [Info] Start training from score -1.051523\nTraining until validation scores don't improve for 200 rounds\n[500]\tvalid_0's auc: 0.957756\tvalid_0's binary_logloss: 0.230936\n[1000]\tvalid_0's auc: 0.959885\tvalid_0's binary_logloss: 0.222709\n[1500]\tvalid_0's auc: 0.960322\tvalid_0's binary_logloss: 0.221306\n[2000]\tvalid_0's auc: 0.960418\tvalid_0's binary_logloss: 0.22097\n[2500]\tvalid_0's auc: 0.960471\tvalid_0's binary_logloss: 0.220788\n[3000]\tvalid_0's auc: 0.960508\tvalid_0's binary_logloss: 0.220659\n[3500]\tvalid_0's auc: 0.96053\tvalid_0's binary_logloss: 0.220605\nEarly stopping, best iteration is:\n[3435]\tvalid_0's auc: 0.960535\tvalid_0's binary_logloss: 0.220589\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Warning] feature_fraction is set=0.9414629297470957, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9414629297470957\n[LightGBM] [Warning] bagging_fraction is set=0.9077035865260106, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9077035865260106\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-02-15 02:12:04,369] Trial 3 finished with value: 0.788040289358146 and parameters: {'n_estimators': 4800, 'num_leaves': 90, 'learning_rate': 0.008385761019753269, 'min_data_in_leaf': 30, 'bagging_fraction': 0.9077035865260106, 'feature_fraction': 0.9414629297470957}. Best is trial 1 with value: 0.7881512757776805.\n","output_type":"stream"},{"name":"stdout","text":"Validation Gini: 0.788040\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/2812364169.py:2: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  n_estimators = trial.suggest_int('n_estimators',1000,5000,200)\n/tmp/ipykernel_33/2812364169.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  num_leaves = trial.suggest_int('num_leaves',10,100,10)\n/tmp/ipykernel_33/2812364169.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  learning_rate = trial.suggest_uniform('learning_rate',1e-5,1e-2)\n/tmp/ipykernel_33/2812364169.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.4, 1.0)\n/tmp/ipykernel_33/2812364169.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  feature_fraction = trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n[LightGBM] [Warning] feature_fraction is set=0.9327032330586474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9327032330586474\n[LightGBM] [Warning] bagging_fraction is set=0.7314635240434976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7314635240434976\n[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n[LightGBM] [Warning] feature_fraction is set=0.9327032330586474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9327032330586474\n[LightGBM] [Warning] bagging_fraction is set=0.7314635240434976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7314635240434976\n[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.142186 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 35440\n[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 150\n[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n[LightGBM] [Warning] feature_fraction is set=0.9327032330586474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9327032330586474\n[LightGBM] [Warning] bagging_fraction is set=0.7314635240434976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7314635240434976\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n[LightGBM] [Info] Start training from score -1.051523\nTraining until validation scores don't improve for 200 rounds\n[500]\tvalid_0's auc: 0.954369\tvalid_0's binary_logloss: 0.260034\n[1000]\tvalid_0's auc: 0.957628\tvalid_0's binary_logloss: 0.231416\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's auc: 0.957628\tvalid_0's binary_logloss: 0.231416\n[LightGBM] [Warning] min_data_in_leaf is set=82, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=82\n[LightGBM] [Warning] feature_fraction is set=0.9327032330586474, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9327032330586474\n[LightGBM] [Warning] bagging_fraction is set=0.7314635240434976, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7314635240434976\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-02-15 02:28:07,975] Trial 4 finished with value: 0.7774755217819116 and parameters: {'n_estimators': 1000, 'num_leaves': 80, 'learning_rate': 0.004157207090213973, 'min_data_in_leaf': 82, 'bagging_fraction': 0.7314635240434976, 'feature_fraction': 0.9327032330586474}. Best is trial 1 with value: 0.7881512757776805.\n","output_type":"stream"},{"name":"stdout","text":"Validation Gini: 0.777476\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/2812364169.py:2: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  n_estimators = trial.suggest_int('n_estimators',1000,5000,200)\n/tmp/ipykernel_33/2812364169.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  num_leaves = trial.suggest_int('num_leaves',10,100,10)\n/tmp/ipykernel_33/2812364169.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  learning_rate = trial.suggest_uniform('learning_rate',1e-5,1e-2)\n/tmp/ipykernel_33/2812364169.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.4, 1.0)\n/tmp/ipykernel_33/2812364169.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  feature_fraction = trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n[LightGBM] [Warning] feature_fraction is set=0.5200237940729856, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5200237940729856\n[LightGBM] [Warning] bagging_fraction is set=0.7058225723186217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7058225723186217\n[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n[LightGBM] [Warning] feature_fraction is set=0.5200237940729856, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5200237940729856\n[LightGBM] [Warning] bagging_fraction is set=0.7058225723186217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7058225723186217\n[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.844071 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 35440\n[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 150\n[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n[LightGBM] [Warning] feature_fraction is set=0.5200237940729856, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5200237940729856\n[LightGBM] [Warning] bagging_fraction is set=0.7058225723186217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7058225723186217\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n[LightGBM] [Info] Start training from score -1.051523\nTraining until validation scores don't improve for 200 rounds\n[500]\tvalid_0's auc: 0.958852\tvalid_0's binary_logloss: 0.227413\n[1000]\tvalid_0's auc: 0.96052\tvalid_0's binary_logloss: 0.221017\n[1500]\tvalid_0's auc: 0.96082\tvalid_0's binary_logloss: 0.220019\n[2000]\tvalid_0's auc: 0.960906\tvalid_0's binary_logloss: 0.219712\n[2500]\tvalid_0's auc: 0.960931\tvalid_0's binary_logloss: 0.219608\n[3000]\tvalid_0's auc: 0.960965\tvalid_0's binary_logloss: 0.219485\nEarly stopping, best iteration is:\n[2898]\tvalid_0's auc: 0.960971\tvalid_0's binary_logloss: 0.219472\n[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n[LightGBM] [Warning] feature_fraction is set=0.5200237940729856, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5200237940729856\n[LightGBM] [Warning] bagging_fraction is set=0.7058225723186217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7058225723186217\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-02-15 02:49:00,443] Trial 5 finished with value: 0.7880550989985144 and parameters: {'n_estimators': 5000, 'num_leaves': 90, 'learning_rate': 0.009749219900979919, 'min_data_in_leaf': 70, 'bagging_fraction': 0.7058225723186217, 'feature_fraction': 0.5200237940729856}. Best is trial 1 with value: 0.7881512757776805.\n","output_type":"stream"},{"name":"stdout","text":"Validation Gini: 0.788055\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/2812364169.py:2: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  n_estimators = trial.suggest_int('n_estimators',1000,5000,200)\n/tmp/ipykernel_33/2812364169.py:3: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n  num_leaves = trial.suggest_int('num_leaves',10,100,10)\n/tmp/ipykernel_33/2812364169.py:4: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  learning_rate = trial.suggest_uniform('learning_rate',1e-5,1e-2)\n/tmp/ipykernel_33/2812364169.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.4, 1.0)\n/tmp/ipykernel_33/2812364169.py:7: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n  feature_fraction = trial.suggest_uniform('feature_fraction', 0.4, 1.0)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n[LightGBM] [Warning] feature_fraction is set=0.9088312175052823, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9088312175052823\n[LightGBM] [Warning] bagging_fraction is set=0.4702465628840158, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4702465628840158\n[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n[LightGBM] [Warning] feature_fraction is set=0.9088312175052823, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9088312175052823\n[LightGBM] [Warning] bagging_fraction is set=0.4702465628840158, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4702465628840158\n[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.126302 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 35440\n[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 150\n[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\n[LightGBM] [Warning] feature_fraction is set=0.9088312175052823, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9088312175052823\n[LightGBM] [Warning] bagging_fraction is set=0.4702465628840158, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4702465628840158\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n[LightGBM] [Info] Start training from score -1.051523\nTraining until validation scores don't improve for 200 rounds\n","output_type":"stream"},{"name":"stderr","text":"[W 2024-02-15 02:49:58,741] Trial 6 failed with parameters: {'n_estimators': 4000, 'num_leaves': 60, 'learning_rate': 0.006311357022953468, 'min_data_in_leaf': 26, 'bagging_fraction': 0.4702465628840158, 'feature_fraction': 0.9088312175052823} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n    value_or_values = func(trial)\n  File \"/tmp/ipykernel_33/2812364169.py\", line 38, in objective\n    gbm = LGBMClassifier(**params).fit(x_train, y_train,\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py\", line 1187, in fit\n    super().fit(\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py\", line 885, in fit\n    self._Booster = train(\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py\", line 276, in train\n    booster.update(fobj=fobj)\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 3891, in update\n    _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\nKeyboardInterrupt\n[W 2024-02-15 02:49:58,748] Trial 6 failed with value None.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[77], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gini_score\n\u001b[1;32m     59\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n","Cell \u001b[0;32mIn[77], line 38\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''X=train.drop(['target'],axis=1)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03my=train['target']\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.3,\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m                                                  shuffle=True,\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m                                                 stratify=y, random_state=42)'''\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mx_train,x_test,y_train,y_test\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    print(\"\\nFold {}\".format(fold+1))\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    X_train, y_train = X.iloc[train_idx,:], y[train_idx]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    X_val, y_val = X.iloc[val_idx,:], y[val_idx]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    #pred_labels = np.rint(preds)'''\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m gbm \u001b[38;5;241m=\u001b[39m \u001b[43mLGBMClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary_logloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m predictions \u001b[38;5;241m=\u001b[39m gbm\u001b[38;5;241m.\u001b[39mpredict_proba(x_test)[:,\u001b[38;5;241m1\u001b[39m]   \n\u001b[1;32m     45\u001b[0m y_pred\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m:predictions})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:1187\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1185\u001b[0m             valid_sets\u001b[38;5;241m.\u001b[39mappend((valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y)))\n\u001b[0;32m-> 1187\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:885\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    882\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    883\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 885\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mbest_iteration\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    269\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m    270\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    271\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    272\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[1;32m    273\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[1;32m    274\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 276\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:3891\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3890\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3891\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3892\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model = LGBMClassifier(boosting_type= 'gbdt',\n              n_estimators = 3400,\n              num_leaves = 100,\n              learning_rate = 0.009116643689805394,\n              min_data_in_leaf = 34,\n              bagging_fraction = 0.9564126735455002,\n              feature_fraction = 0.8576917706000423,\n              #'max_bins': 500,\n              #'reg_alpha': 2,\n              objective = 'binary',\n              metric = 'binary_logloss',\n              device = 'cpu',\n              random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T15:21:12.609665Z","iopub.execute_input":"2024-05-22T15:21:12.610100Z","iopub.status.idle":"2024-05-22T15:21:12.617648Z","shell.execute_reply.started":"2024-05-22T15:21:12.610066Z","shell.execute_reply":"2024-05-22T15:21:12.616306Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset[num_columns], train_dataset['target'])","metadata":{"execution":{"iopub.status.busy":"2024-05-22T15:21:25.014703Z","iopub.execute_input":"2024-05-22T15:21:25.015193Z","iopub.status.idle":"2024-05-22T15:43:10.602785Z","shell.execute_reply.started":"2024-05-22T15:21:25.015158Z","shell.execute_reply":"2024-05-22T15:43:10.601514Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n[LightGBM] [Warning] feature_fraction is set=0.8576917706000423, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8576917706000423\n[LightGBM] [Warning] bagging_fraction is set=0.9564126735455002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9564126735455002\n[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n[LightGBM] [Warning] feature_fraction is set=0.8576917706000423, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8576917706000423\n[LightGBM] [Warning] bagging_fraction is set=0.9564126735455002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9564126735455002\n[LightGBM] [Info] Number of positive: 118828, number of negative: 340085\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.823688 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 35442\n[LightGBM] [Info] Number of data points in the train set: 458913, number of used features: 150\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258934 -> initscore=-1.051519\n[LightGBM] [Info] Start training from score -1.051519\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"LGBMClassifier(bagging_fraction=0.9564126735455002, device='cpu',\n               feature_fraction=0.8576917706000423,\n               learning_rate=0.009116643689805394, metric='binary_logloss',\n               min_data_in_leaf=34, n_estimators=3400, num_leaves=100,\n               objective='binary', random_state=42)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(bagging_fraction=0.9564126735455002, device=&#x27;cpu&#x27;,\n               feature_fraction=0.8576917706000423,\n               learning_rate=0.009116643689805394, metric=&#x27;binary_logloss&#x27;,\n               min_data_in_leaf=34, n_estimators=3400, num_leaves=100,\n               objective=&#x27;binary&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(bagging_fraction=0.9564126735455002, device=&#x27;cpu&#x27;,\n               feature_fraction=0.8576917706000423,\n               learning_rate=0.009116643689805394, metric=&#x27;binary_logloss&#x27;,\n               min_data_in_leaf=34, n_estimators=3400, num_leaves=100,\n               objective=&#x27;binary&#x27;, random_state=42)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"predictions = model.predict_proba(test_dataset[num_columns])\npredictions","metadata":{"execution":{"iopub.status.busy":"2024-05-22T15:46:19.192310Z","iopub.execute_input":"2024-05-22T15:46:19.192820Z","iopub.status.idle":"2024-05-22T15:55:53.484201Z","shell.execute_reply.started":"2024-05-22T15:46:19.192784Z","shell.execute_reply":"2024-05-22T15:55:53.482957Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n[LightGBM] [Warning] feature_fraction is set=0.8576917706000423, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8576917706000423\n[LightGBM] [Warning] bagging_fraction is set=0.9564126735455002, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9564126735455002\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"array([[0.98136738, 0.01863262],\n       [0.99818394, 0.00181606],\n       [0.96647382, 0.03352618],\n       ...,\n       [0.48189942, 0.51810058],\n       [0.79202667, 0.20797333],\n       [0.96231145, 0.03768855]])"},"metadata":{}}]},{"cell_type":"code","source":"predictions[:,1]","metadata":{"execution":{"iopub.status.busy":"2024-05-22T16:40:01.239649Z","iopub.execute_input":"2024-05-22T16:40:01.240465Z","iopub.status.idle":"2024-05-22T16:40:01.253496Z","shell.execute_reply.started":"2024-05-22T16:40:01.240414Z","shell.execute_reply":"2024-05-22T16:40:01.251957Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"array([0.01863262, 0.00181606, 0.03352618, ..., 0.51810058, 0.20797333,\n       0.03768855])"},"metadata":{}}]},{"cell_type":"code","source":"sample_dataset = pd.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\noutput = pd.DataFrame({'customer_ID': sample_dataset.customer_ID, 'prediction': predictions[:,1]})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T16:41:06.335100Z","iopub.execute_input":"2024-05-22T16:41:06.335580Z","iopub.status.idle":"2024-05-22T16:41:14.004043Z","shell.execute_reply.started":"2024-05-22T16:41:06.335480Z","shell.execute_reply":"2024-05-22T16:41:14.002633Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2024-02-15T04:48:37.049798Z","iopub.execute_input":"2024-02-15T04:48:37.050339Z","iopub.status.idle":"2024-02-15T04:48:37.066148Z","shell.execute_reply.started":"2024-02-15T04:48:37.050302Z","shell.execute_reply":"2024-02-15T04:48:37.064820Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"                                              customer_ID  prediction\n0       00000469ba478561f23a92a868bd366de6f6527a684c9a...    0.018633\n1       00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...    0.001816\n2       0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...    0.033526\n3       00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...    0.247748\n4       00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...    0.929911\n...                                                   ...         ...\n924616  ffff952c631f2c911b8a2a8ca56ea6e656309a83d2f64c...    0.012681\n924617  ffffcf5df59e5e0bba2a5ac4578a34e2b5aa64a1546cd3...    0.752047\n924618  ffffd61f098cc056dbd7d2a21380c4804bbfe60856f475...    0.518101\n924619  ffffddef1fc3643ea179c93245b68dca0f36941cd83977...    0.207973\n924620  fffffa7cf7e453e1acc6a1426475d5cb9400859f82ff61...    0.037689\n\n[924621 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a...</td>\n      <td>0.018633</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...</td>\n      <td>0.001816</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...</td>\n      <td>0.033526</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...</td>\n      <td>0.247748</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...</td>\n      <td>0.929911</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>924616</th>\n      <td>ffff952c631f2c911b8a2a8ca56ea6e656309a83d2f64c...</td>\n      <td>0.012681</td>\n    </tr>\n    <tr>\n      <th>924617</th>\n      <td>ffffcf5df59e5e0bba2a5ac4578a34e2b5aa64a1546cd3...</td>\n      <td>0.752047</td>\n    </tr>\n    <tr>\n      <th>924618</th>\n      <td>ffffd61f098cc056dbd7d2a21380c4804bbfe60856f475...</td>\n      <td>0.518101</td>\n    </tr>\n    <tr>\n      <th>924619</th>\n      <td>ffffddef1fc3643ea179c93245b68dca0f36941cd83977...</td>\n      <td>0.207973</td>\n    </tr>\n    <tr>\n      <th>924620</th>\n      <td>fffffa7cf7e453e1acc6a1426475d5cb9400859f82ff61...</td>\n      <td>0.037689</td>\n    </tr>\n  </tbody>\n</table>\n<p>924621 rows × 2 columns</p>\n</div>"},"metadata":{}}]}]}